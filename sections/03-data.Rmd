# Materials and  Methodology

## Reproducable research and data science with R
Reproducibility or reproducible research is a major principle of reliable scientific methodology and is becoming increasingly popular and common practice with advancing computer technology, open source programming languages and fast self-developed statistical analysis. For the findings of a study to be found reproducible means that results obtained by an experiment or an observational study or in a statistical analysis of a data set should be achieved again with a high degree of reliability when the study is replicated. For the preparation and the analysis of the data included in this thesis, "R" which is an open source programming language and environment for statistical computing version 4.0.5 (2021-03-31) -- "Shake and Throw" and RStudio Version 1.4.1106, which is the associated integrated development environment (IDE), is used @R. Extensions of the actual language are called packages of those the following are in use: rio, data.table, tidyverse, magrittr, compare, leaflet, sp, raster, rgdal, htmltools, htmlwidgets, tmaptools, maptools, raster, sf, tmap, spatstat, rgeos and diagrammR. The data handling is mostly done with base R and deplyr from the tidyverse package collection. Deplyr is a grammar of data manipulation, providing a consistent set of verbs that help performing common data manipulation challenges @HadleyWickham.2021. All the data preparation, filtering, merging and formatting in this work was performed using base R or dplyr.

For the purpose of education and reproducibility, which are prerequisites for this study, the whole work was written with the programming language R. To be more precise the statistical analysis was done in R and the thesis itself was written in R Markdown. In essence R Markdown stands on the shoulders of knitr, which is an R package and translates all sorts of code, mainly the R code itself into markdown code @YihuiXieJ.J.AllaireGarrettGrolemund.2021. Pandoc, which is a free and open-source document converter, then translates this code into an output format such as HTML, PDF or Word. In this case the R Markdown code is translated first into a LaTeX form which then is further transferred into a PDF output. Markdown itself is a markup language that is widely used in blogging, instant messaging, online forums and readme files. LaTeX on the other hand is a software system that is used for document preparation and is also sometimes referred to as literate programming because the user types plain text and code elements for specifying the formatting of the document in contrast to the “what you see is what you get” and drag and drop principle of word processors like Microsoft Word or Libre Office @LaTeX.2021. The formatting and layout of this master thesis comes from a package of the Humboldt-University of Berlin (HU) which offers a template for a thesis according to the regulations of HU.  This package is related to the bookdown package, which is a template structure for writing books, ebooks, manuscripts, HTML documents and more. Due to exhaustive code only the line chart outputs are shown in this work as well as some important pieces of code in the appendix. The full documentation, especially the preparation steps can be found on [https://github.com/EliasCuadra/master-thesis-repowering](https://github.com/EliasCuadra/master-thesis-repowering) with explanatory README.md files that contain descriptions of variables and processing steps. Git, which is a version control software, was used initially to create several versions of the same chunk of code and ultimately to create a backup in the cloud. When this work is to be completely comprehended and reproduced or the methodology should be applied in another context or is to be adopted in another state it is definitely recommended to look into the GitHub folder since the majority of the time for this work was actually spend creating all the code and files needed for the analysis, which is not necessarily reflected in the written thesis.


## Regression analysis
A regression analysis is used to check whether there is a relationship between the values of two or more variables and how strong this relation is. In the context of the data evaluation of the amount of electricity fed into the grid from WTs, the amount of electricity, the installed nominal power and the energy density depending on the commissioning date are of particular interest in order to be able to make statements about future developments. This relationship is tested in a regression analysis in the form of a comparison that measures the change in the dependent variable when the value of the independent variable increases or decreases by the value 1. Since this study uses only a very simple statistical analysis, only a simple linear regression with two variables is used, which in some cases is supplemented by a third-order polynomial equation to simulate disproportionate growth. The procedure is that from the data points, in this case the electricity yield or rated capacity per WT in relation to the commissioning date, that linear function equation is selected which makes the sum of the squares of the deviations, also referred to as the residuals, from the data points to this exact line is as small as possible. This is also why linear regression is also called *least-squares estimation* @NormanMatloff.2017. A simple overview about a chart that shows a linear regression output with explanations is show in Figure \@ref(fig:regression).

(ref:regr) Basic representation of linear regression analysis with date points, regression line and confidence intervals @JasonWong.2020. 


```{r regression, echo=FALSE, fig.cap="(ref:regr)", out.width = "100%"}
knitr::include_graphics("figures/regression.jpeg")
```


To explain some fundamentals, the variance in statistics is the expectation of the squared deviation of a random variable from its population mean or sample mean. Variance is therefore a measure of dispersion, meaning it is a measure of how far a set of numbers is spread out from their average value. The R-squared (R²) value in this context is the proportion by which the variance of the dependent variable is explained by the variance of the independent variable. This should not be mistaken for the strength of the overall relationship which would be the correlation. For example if R² is 0.5 this means that 50% of the variation in the data can be explained by the variation of the input data. The results in Chapter 4 are also presented with a p-value. The p-value is the probability that random chance created the results that were observed or something that is even more extreme and is therefore a measure of the statistical significance. The lower the p-value the greater the significance @LeoH.Kahane.2006. To find outliers and clean the data Cook’s distance is used in this work which is a commonly used estimate of the influence of a data point when performing a regression analysis. All the statistical methods are carried out with the R programming language and all outputs and diagnostic plots did not show any critical structures within the data. Further explanation of statistical procedures is therefore avoided and also regarded as common sense. In the following example code it is shown how a linear model of the electricity yield in MWh as *menge_mwh* over the commissioning date as *inbetriebnahme* from the amprion data set is created using the lm() function, subsequently the summary function is called to see the significance and the R² value of the model and finally the diagnostic plots of the model are printed to check the model assumptions of a linear regression model such as normal distribution, independence of error, homogeneity of the variance and that no outliers are detected with Cook's distance (Figure \@ref(fig:diagnostics)).


```{r diagnostics, echo=FALSE, fig.cap="Example diagnostic plot of linear model of the electricity yield of all WTs in RLP over the commissioning date in 2019 from the transmition system operator Amprion created with the plot() function", out.width = "100%", fig.pos="H"}
knitr::include_graphics("figures/diagnostics.png")
```

The residual vs fitted values plot in the upper left reveals if the residuals are equally distributed among the outcome variables or have a non-linear pattern. In this case the residuals become larger with increasing fitted values which indicates a increasing error. The normal QQ plot in the upper right shows that the data is normally distributed if the dots follow the straight line. In this case it small and high values stray from the line. The scale-location plot in the lower left similar to the first plot shows whether the assumption of equal variance (homoscedasticity) is matched if there is a straight line with equally spread points. The residuals vs leverage plot in the lower right helps to find influential points. When points are outside Cook's distance (red dotted line - not visible here) they are considered to be influential and can be removed from the data, although every case and data set has to be judged individually @BommaeKim.2015.

## Geospatial analysis and area calculation with R

For the estimation of the distance between WTs in RLP and the subsequent calculation of the area consumption, a short geospatial analysis of the data was carried out. The data contained geographic references in a spherical coordinate system using longitude and latitude values called World Geodic System 1984 (WGS84). A Geographic Reference System (GEOREF) or coordinate reference system (CRS) is a geocode or coordinate-based local, regional or global system used to locate geographical entities or specifying locations on the surface of the earth @ISO.2007. To perform the analysis the R packages sp, sf, spatstats, rgeos and rgdal were used. In particular or as a central operation the nndist() function was used to calculate the nearest neighbor of each WT.  The reason why R is used for this an all other purposes in this work is because it provides freely accessible packages e.g. for spatial analysis that provide all the necessary tools for checking, manipulating and analyzing data @RogerS.Bivan.2008.

## Master and Movement data from Amprion

The so-called master and movement data from the transmission system operator Amprion of the years 2012 to 2019 was available and used for analysis @EnergieagenturRheinlandPfalz.2021. Despite the availability of older data sets, in some cases only more recent years are used since it is assumed, that the usage of older data would, due to fast technical development, only distort the results in regards to future prediction. The original two data sets were already joined and prepared by the energy agency of RLP. The master data contains information about the location, commissioning and decommissioning date and the EEG system key of all EEG systems that were reported by the distribution network operator as part of the annual EEG report. The files also contain the systems connected directly or indirectly to the networks of the transmission system operator. Additionally, the transmission system operators are obliged to publish the so-called movement data that contains information about the quantity of electricity fed in and the amount of funded tariffs that have been payed @AmprionGmbH.2019. The pre-processed data sets from the years 2015 to 2019 are stored within this project in data/amprion as EEG_StammBew_2015( - 2019)_Amprion-EAtlas.csv.

Further, The data from Amprion comes with 28 variables for the year 2019 of which only some are interesting for the analysis, some variable formats are not suitable and distorting data points are included. Thus, a preparation process must initially be carried out to clean and prepare the data. The kept variables are: community key, post code, place, rated capacity, commissioning date, system key and electricity yield. Since it is considered best practice to examine the data for influential points and reliability this is also thoroughly done here @JasonW.Osborne.2013. For the prediction  of the electricity yield not all the WTs are considered meaningful and some outliers are removed from the data. On the one hand WTs with a rated capacity lower than 100 kW are considered too small for a reasonable assessment of new commercially operated systems. On the other hand some WTs are listed with a rated capacity up to 12 MW which might be because several WTs are registered as one. It is therefore assumed that a rated capacity larger than 4.5 MW does not reflect the average conditions, even though systems of this size exist. Also WTs that are built after February 15th each year are excluded since they have less time to generate electricity and therefore deliver misleading data. An interim result are five CSV files of the different years that only have relevant variables in the correct format and are free of outliers. As an insight the data from 2017 shows 1,623 WTs of which 101 are outliers, the data from 2018 shows 1,674 WTs of which 76 are outliers and the data from 2019 shows 1,702 WTs of which 46 are outliers. A plot that shows the total number of WTs from 2012 to 2019 is shown in Figure \@ref(fig:yearswts), which contains, as a foresight on the results section, a linear trend, that shows an estimated number of around 2,500 WTs in the year 2030 .

```{r yearswts, echo=FALSE, fig.cap="Number of total WTs in RLP from 2012 to 2019 according to Amprion with linear trend up to 2030", out.width = "100%", fig.pos="H"}
knitr::include_graphics("data/Amprion/results_of_analysis/year_wts.png")
```


Only the data from 2019 is further processed and variables for the county and municipality names as well as the electricity yield in MWh and the full-load-hours, resulting from the division of electricity yield by rated capacity, are added. The data from 2019 is then divided in three files according to the commissioning date to find the WTs that are built either before 2005, before 2001 or between 2001 and 2005 and are therefore potentially suitable for a repowering process. A flow chart of the individual processing steps that have been performed is presented in Figure \@ref(fig:preparation). The preparation was done to fulfill the main goal of analyzing the master and movement data regarding the electricity yield, rated capacity and the full-load-hours mainly in relation to the commissioning date. Amprion lists 1,702 WTs in RLP in 2019. In comparison to that, the Structure and Approval Directorate North registers 1,704 WTs with technical details and coordinate references that are connected to the public grid. This data set is not further used since it misses the EEG system key and can therefore not be joined with the other data sets, that is presented next.

```{r preparation, echo=FALSE, fig.cap="Flow chart of the cleaning and proccessing steps of the data from Amprion", out.width = "100%"}
knitr::include_graphics("data/Amprion/results_of_preparation/prepflow.png")
```

After the preparation, the data is firstly analyzed regarding the amount of electricity generated per WT and per year. For this purpose, a linear and a 3rd order polynomial regression, which allow for a forecast until 2030, is carried out. The amount of electricity fed in, here referred to as the electricity yield, is the dependent variable and the commissioning date as the independent variable. Secondly the electricity yield is plotted over the rated capacity and henceforth only linear trends are applied since polynomial models did not create a better fit and sometimes even a decrease for the future. Additionally a plot of the rated capacity over the commissioning date is generated. For this only data from 2019 is used since the rated capacity of the already installed WTs do not change over time. Further the full-load-hours as a concept of the degree of utilization is analyzed in regard to the WTs commissioning date. The degree of utilization might be more dependent on the wind conditions than the technical development and is therefore specifically good for predictions and therefore data from the past 5 years is analyzed. In addition to the wind conditions the full-load-hours depend on the specific nominal output, which means the rotor diameter, and hence the technological development cannot be disregarded. This means that old data might still deliver a wrong impression on future predictions but also might account for weaker years in terms of wind conditions since 2019 might be a stronger wind year. The Code that was used to perform the analysis can be found in the appendix as A.1 Code chunk 1.

Since it is crucial not only to know the electricity yield per WT but also the electricity yield per area to estimate how much of the area of Rhineland-Palatinate must actually be covered with WTs in order to achieve the target of 22 TWh of wind energy per year in 2030. Therefore data from the master data register that contains geographic information about the WTs is used to calculate their area consumption and that data set and the respective process is described in the next subsection.

## Data from the market master data register

The market master data register (Marktstammdatenregister - MaStR) contains data about all electricity and gas power generation units and can be freely accessed here:

[https://www.marktstammdatenregister.de/MaStR/Einheit/Einheiten/
OeffentlicheEinheitenuebersicht](https://www.marktstammdatenregister.de/MaStR/Einheit/Einheiten/OeffentlicheEinheitenuebersicht)

The electricity units with the generation type "wind" for Rhineland-Palatinate that are in operation are extracted on the 23th of July 2021 as MaStR.csv and stored in this project in data/MaStR_amprion. This data set counts 1,752 WTs with 50 variables of which only 8 are kept: the rated capacity, the commissioning date, the community key, the coordinates (CRS: WGS84), hub height, rotor diameter and EEG system key. It does not contain the amount of electricity fed into the public grid. Thus, to calculate the electricity generation per area the data of the MaStR and Amprion needs to be joined together according to the EEG system key. The process of preparing the MaStR data and joining it with the data from Amprion of 2019 is shown as a flow chart in Figure \@ref(fig:merge). The R code is stored as MaStR_amprion.R in the MaStR_amprion folder. 

```{r merge, echo=FALSE, fig.cap="Flow chart of merging process of the data set from Amprion of 2019 and the data from the MaStR", out.width = "80%", fig.pos="H"}
knitr::include_graphics("data/MaStR_amprion_join/mergeflow.png")
```

The result are 1,620 WTs that could be correctly assigned and are ready for further analysis and stored as MaStR_amprion2019.csv in the same source location. After cleaning the data from WTs that were probably wrongly located outside of RLP 1,589 WTs remained. For the analysis of the development of the rotor diameter, hub height and area consumption a nearest neighbor analysis is performed to find the distance between WTs. The Code that is used to do the geospatial distance estimation is shown in the appendix as A.2 Code chunk 2. A flow chart that shows the processing steps is shown in Figure \@ref(fig:nearest). From knowing the distance to the nearest neighboring WT the area occupied by the WT is calculated as the square of that distance $d^2$ being the area covered by a rectangle around the WT. This also takes into account that the area that must be shown in plans and maps are usually greater than the actual occupied area which might be more accurate by calculating a circle around the WT. The electricity yield over the area consumption then reveals the energy density in kWh/m².

```{r nearest, echo=FALSE, fig.cap="Flow chart of the geospatial processing", out.width = "100%", fig.pos="H"}
knitr::include_graphics("figures/flowchart_nearest.png")
```

Further, the data is cleaned from data points with no value for the electricity yield available as well as outliers. For example rotor diameters as well as hub heights over 200 m are considered unrealistically high for the calculation of a average value. For the area consumption it is assumed that if the distance to the next WT is grater than 1,000 m it is not because of shadowing effects. Also when the value is 0 it is deleted from the data. WTs with a electricity yield of less than 20 MWh and a distance of less than 30 m are also deleted. Also a energy density of more than 500 kWh/m² is regarded as too high. Similarly to the procedure that was performed to the data from Amprion and described above, a regression analysis to find a prediction for the area consumption if 22 TWh should come from wind energy in the year of 2030 is done. Respectively the Code can be found in the appendix as A.3 Code chunk 3.















\newpage






  



